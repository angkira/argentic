# filepath: /mnt/c/Users/angkira/Project/services/rag/config.yaml
llm:
  # backend: 'llamaserver' # Choose 'ollama', 'llamacpp', or 'llamaserver'
  backend: 'ollama'
  model_name: 'gemma3:12b-it-qat'
  # llama.cpp HTTP server
  server_binary: '~/llama.cpp/build/bin/llama-server' # Updated binary path
  server_args:
    - '--host'
    - '127.0.0.1'
    - '--port'
    - '5000'
    - '-m'
    - '~/llama.cpp/models/gemma-3-12b-it-q4_0.gguf'
  server_host: '127.0.0.1'
  server_port: 5000
  # Optional direct llama.cpp CLI binary (for 'llamacpp' backend)
  llama_cpp_binary: '~/llama.cpp/build/bin/llama-gemma3-cli'
  llama_cpp_args:
    - '--temp'
    - '0.7'
    - '--n'
    - '128'
  use_chat: true # enable session-based chat endpoint

embedding:
  model_name: 'all-MiniLM-L6-v2' # Or all-mpnet-base-v2
  device: 'cuda' # "cuda" or "cpu"
  normalize: true

vector_store:
  directory: './pc_knowledge_db'
  collection_name: 'local_info'

retriever:
  k: 4 # Number of documents to retrieve

mqtt:
  broker_address: 'localhost' # Or IP address of your Mosquitto broker
  port: 1883
  client_id: 'rag_agent_client'
  keepalive: 60
  subscriptions: # Topics to subscribe to and their handlers
    'rag/command/add_info': 'handle_add_info'
    'rag/command/ask_question': 'handle_ask_question'
    'rag/status/request': 'handle_status_request'
  publish_topics: # Topics to publish responses/status to
    response: 'rag/response/answer'
    status: 'rag/status/info'
    log: 'rag/log'
