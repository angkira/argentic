# filepath: /mnt/c/Users/angkira/Project/services/rag/config.yaml
llm:
  backend: ollama
  model_name: gemma3:12b-it-qat
  server_binary: ~/llama.cpp/build/bin/llama-server
  server_args:
    - --host
    - 127.0.0.1
    - --port
    - 5000
    - -m
    - ~/llama.cpp/models/gemma-3-12b-it-q4_0.gguf
  server_host: 127.0.0.1
  server_port: 5000
  llama_cpp_binary: ~/llama.cpp/build/bin/llama-gemma3-cli
  llama_cpp_args:
    - --temp
    - 0.7
    - --n
    - 128
  use_chat: true

embedding:
  model_name: all-MiniLM-L6-v2
  device: cuda
  normalize: true

vector_store:
  directory: ./pc_knowledge_db
  collection_name: local_info

retriever:
  k: 4

mqtt:
  broker_address: localhost
  port: 1883
  client_id: rag_agent_client
  keepalive: 60
  subscriptions:
    rag/command/add_info: handle_add_info
    rag/command/ask_question: handle_ask_question
    rag/status/request: handle_status_request
  publish_topics:
    response: agent/response/answer
    status: agent/status/info
    log: agent/log
