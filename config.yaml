# filepath: /mnt/c/Users/angkira/Project/services/rag/config.yaml
llm:
  backend: ollama
  model_name: gemma3:12b-it-qat
  server_binary: ~/llama.cpp/build/bin/llama-server
  server_args:
    - --host
    - 127.0.0.1
    - --port
    - 5000
    - -m
    - ~/llama.cpp/models/gemma-3-12b-it-q4_0.gguf
  server_host: 127.0.0.1
  server_port: 5000
  llama_cpp_binary: ~/llama.cpp/build/bin/llama-gemma3-cli
  llama_cpp_args:
    - --temp
    - 0.7
    - --n
    - 128
  use_chat: true

# RAG configuration is now in src/tools/RAG/rag_config.yaml

mqtt:
  broker_address: localhost
  port: 1883
  client_id: ai_agent_client
  tool_client_id: ai_tool_service
  keepalive: 60
  subscriptions:
    agent/command/ask_question: handle_ask_question
    agent/status/request: handle_status_request
  publish_topics:
    response: agent/response/answer
    status: agent/status/info
    log: agent/log
