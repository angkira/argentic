# Visual Agent with Gemma 3n Configuration
#
# This configuration file sets up a visual AI agent using:
# - Gemma 3n E4B multimodal model (text, images, audio)
# - WebRTC for low-latency video/audio capture
# - MQTT for response distribution
#
# Requirements:
# - Gemma 3n model checkpoint downloaded
# - MQTT broker (mosquitto) running
# - GPU recommended for optimal performance

# LLM Configuration - Gemma 3n E4B
llm:
  provider: gemma
  
  # Model selection
  gemma_model_name: gemma-3n-e4b-it  # or gemma-3n-e2b-it for smaller model
  
  # Checkpoint path (can be Kaggle path or local directory)
  # Download from: https://www.kaggle.com/models/google/gemma-3n/
  gemma_checkpoint_path: GEMMA3_4B_IT  # or full path like /path/to/checkpoint
  
  # Enable PLE (Per-Layer Embedding) caching for memory efficiency
  gemma_enable_ple_caching: true
  
  # Enable multi-turn conversation
  gemma_multi_turn: true
  
  # Generation parameters
  gemma_parameters:
    temperature: 0.7        # Creativity (0.0-1.0)
    top_p: 0.95             # Nucleus sampling
    max_output_tokens: 2048 # Maximum response length

# Messaging Configuration - MQTT
messaging:
  protocol: mqtt
  broker_address: localhost  # MQTT broker address
  port: 1883                 # MQTT port
  keepalive: 60
  client_id: ""              # Auto-generated if empty
  username: null             # Optional authentication
  password: null

# WebRTC Configuration
webrtc:
  # Signaling server URL (optional, depends on your WebRTC setup)
  signaling_url: null  # e.g., "wss://your-server.com/signal"
  
  # ICE servers for NAT traversal
  ice_servers:
    - urls: "stun:stun.l.google.com:19302"
    # Add TURN servers if needed:
    # - urls: "turn:your-turn-server.com:3478"
    #   username: "user"
    #   credential: "pass"
  
  # Video buffering
  video_buffer_size: 30       # Number of frames to buffer
  frame_rate: 10              # Target frames per second
  resize_frames: [640, 480]   # Resize frames [width, height] for efficiency
  frame_format: "rgb24"       # Frame format
  
  # Audio buffering
  audio_buffer_duration: 5.0  # Seconds of audio to buffer
  audio_sample_rate: 16000    # Audio sample rate (Hz)
  enable_audio: true          # Enable audio capture
  
  # Threading
  frame_processor_workers: 4  # Thread pool size for frame processing

# Visual Agent Configuration
visual_agent:
  # Auto-processing
  auto_process_interval: 5.0         # Process buffer every N seconds
  enable_auto_processing: true       # Enable automatic processing
  min_frames_for_processing: 10      # Minimum frames before processing
  
  # Response publishing
  visual_response_topic: "agent/visual/response"  # MQTT topic for responses
  
  # Prompts
  visual_prompt_template: "Describe what you see: {question}"
  system_prompt: |
    You are a helpful visual AI assistant with the ability to see and hear.
    Describe what you observe in the video clearly and concisely.
    If you hear audio, incorporate that information into your responses.
    Be specific about objects, actions, and scenes you detect.

# Agent Base Configuration
agent:
  role: "visual_assistant"
  description: "Visual AI agent with video and audio processing capabilities"
  enable_dialogue_logging: true
  expected_output_format: "text"
  
  # Context management
  max_dialogue_history_items: 100
  max_query_history_items: 20
  
  # Tool loop detection
  adaptive_max_iterations: true
  max_consecutive_tool_calls: 3
  enable_completion_analysis: true

# Logging Configuration
logging:
  level: INFO  # DEBUG, INFO, WARNING, ERROR
  
# Topics Configuration
topics:
  # Visual agent specific
  visual:
    response: "agent/visual/response"
    status: "agent/visual/status"
  
  # Standard agent topics
  agent:
    tasks: "agent/{role}/tasks"
    results: "agent/{role}/results"
    status: "agent/status/info"

